{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 19:05:23.030430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 19:05:23.768075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# U-Net model\n",
    "# coded by st.watermelon\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n",
    "    add, multiply\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow.keras as K\n",
    "\n",
    "def up_and_concate(down_layer, layer, data_format='channels_first'):\n",
    "    if data_format == 'channels_first':\n",
    "        in_channel = down_layer.get_shape().as_list()[1]\n",
    "    else:\n",
    "        in_channel = down_layer.get_shape().as_list()[3]\n",
    "\n",
    "    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n",
    "    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        my_concat = Lambda(lambda x: layers.concatenate([x[0], x[1]], axis=1))\n",
    "    else:\n",
    "        my_concat = Lambda(lambda x: layers.concatenate([x[0], x[1]], axis=3))\n",
    "\n",
    "    concate = my_concat([up, layer])\n",
    "\n",
    "    return concate\n",
    "\n",
    "\n",
    "def attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n",
    "    if data_format == 'channels_first':\n",
    "        in_channel = down_layer.get_shape().as_list()[1]\n",
    "    else:\n",
    "        in_channel = down_layer.get_shape().as_list()[3]\n",
    "\n",
    "    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n",
    "    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n",
    "\n",
    "    layer = attention_block_2d(x=layer, g=up, inter_channel=in_channel // 4, data_format=data_format)\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        my_concat = Lambda(lambda x: layers.concatenate([x[0], x[1]], axis=1))\n",
    "    else:\n",
    "        my_concat = Lambda(lambda x: layers.concatenate([x[0], x[1]], axis=3))\n",
    "\n",
    "    concate = my_concat([up, layer])\n",
    "    return concate\n",
    "\n",
    "\n",
    "def attention_block_2d(x, g, inter_channel, data_format='channels_first'):\n",
    "    # theta_x(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n",
    "\n",
    "    # phi_g(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n",
    "\n",
    "    # f(?,g_height,g_width,inter_channel)\n",
    "\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "\n",
    "    # psi_f(?,g_height,g_width,1)\n",
    "\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n",
    "\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "\n",
    "    # rate(?,x_height,x_width)\n",
    "\n",
    "    # att_x(?,x_height,x_width,x_channel)\n",
    "\n",
    "    att_x = multiply([x, rate])\n",
    "\n",
    "    return att_x\n",
    "\n",
    "\n",
    "def res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n",
    "\n",
    "              padding='same', data_format='channels_first'):\n",
    "    if data_format == 'channels_first':\n",
    "        input_n_filters = input_layer.get_shape().as_list()[1]\n",
    "    else:\n",
    "        input_n_filters = input_layer.get_shape().as_list()[3]\n",
    "\n",
    "    layer = input_layer\n",
    "    for i in range(2):\n",
    "        layer = Conv2D(out_n_filters // 4, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n",
    "        if batch_normalization:\n",
    "            layer = BatchNormalization()(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = Conv2D(out_n_filters // 4, kernel_size, strides=stride, padding=padding, data_format=data_format)(layer)\n",
    "        layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n",
    "\n",
    "    if out_n_filters != input_n_filters:\n",
    "        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n",
    "            input_layer)\n",
    "    else:\n",
    "        skip_layer = input_layer\n",
    "    out_layer = add([layer, skip_layer])\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\n",
    "def rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n",
    "\n",
    "                  padding='same', data_format='channels_first'):\n",
    "    if data_format == 'channels_first':\n",
    "        input_n_filters = input_layer.get_shape().as_list()[1]\n",
    "    else:\n",
    "        input_n_filters = input_layer.get_shape().as_list()[3]\n",
    "\n",
    "    if out_n_filters != input_n_filters:\n",
    "        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n",
    "            input_layer)\n",
    "    else:\n",
    "        skip_layer = input_layer\n",
    "\n",
    "    layer = skip_layer\n",
    "    for j in range(2):\n",
    "\n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "\n",
    "                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n",
    "                    layer)\n",
    "                if batch_normalization:\n",
    "                    layer1 = BatchNormalization()(layer1)\n",
    "                layer1 = Activation('relu')(layer1)\n",
    "            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n",
    "                add([layer1, layer]))\n",
    "            if batch_normalization:\n",
    "                layer1 = BatchNormalization()(layer1)\n",
    "            layer1 = Activation('relu')(layer1)\n",
    "        layer = layer1\n",
    "\n",
    "    out_layer = add([layer, skip_layer])\n",
    "    return out_layer\n",
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    flatten_layer = K.layers.Flatten()\n",
    "    y_true_f = flatten_layer(y_true)\n",
    "    y_pred_f = flatten_layer(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "########################################################################################################\n",
    "# Define the neural network\n",
    "def unet(img_w, img_h, n_label, data_format='channels_first'):\n",
    "    inputs = Input(( img_w, img_h,3))\n",
    "    x = inputs\n",
    "    depth = 4\n",
    "    features = 64\n",
    "    skips = []\n",
    "    for i in range(depth):\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        skips.append(x)\n",
    "        x = MaxPooling2D((2, 2), data_format= data_format)(x)\n",
    "        features = features * 2\n",
    "\n",
    "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "\n",
    "    for i in reversed(range(depth)):\n",
    "        features = features // 2\n",
    "        # attention_up_and_concate(x,[skips[i])\n",
    "        x = UpSampling2D(size=(2, 2), data_format=data_format)(x)\n",
    "        x = concatenate([skips[i], x], axis=1)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "\n",
    "    conv6 = Conv2D(n_label, (1, 1), padding='same', activation='sigmoid',data_format=data_format)(x)\n",
    "    model = Model(inputs=inputs, outputs=conv6)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n",
    "    return model\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "#Attention U-Net\n",
    "def att_unet(img_w, img_h, n_label, data_format='channels_first'):\n",
    "    inputs = Input(( img_w, img_h,3))\n",
    "    x = inputs\n",
    "    depth = 4\n",
    "    features = 64\n",
    "    skips = []\n",
    "    for i in range(depth):\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        skips.append(x)\n",
    "        x = MaxPooling2D((2, 2), data_format='channels_first')(x)\n",
    "        features = features * 2\n",
    "\n",
    "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "\n",
    "    for i in reversed(range(depth)):\n",
    "        features = features // 2\n",
    "        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
    "\n",
    "    conv6 = Conv2D(n_label, (1, 1), padding='same', activation='sigmoid',data_format=data_format)(x)\n",
    "    model = Model(inputs=inputs, outputs=conv6)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n",
    "    return model\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "#Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\n",
    "def r2_unet(img_w, img_h, n_label, data_format='channels_first'):\n",
    "    inputs = Input(( img_w, img_h,3))\n",
    "    x = inputs\n",
    "    depth = 4\n",
    "    features = 20\n",
    "    skips = []\n",
    "    for i in range(depth):\n",
    "        x = rec_res_block(x, features, data_format=data_format)\n",
    "        skips.append(x)\n",
    "        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n",
    "\n",
    "        features = features * 2\n",
    "\n",
    "    x = rec_res_block(x, features, data_format=data_format)\n",
    "\n",
    "    for i in reversed(range(depth)):\n",
    "        features = features // 2\n",
    "        x = up_and_concate(x, skips[i], data_format=data_format)\n",
    "        x = rec_res_block(x, features, data_format=data_format)\n",
    "\n",
    "    conv6 = Conv2D(n_label, (1, 1), padding='same', activation='sigmoid',data_format=data_format)(x)\n",
    "    model = Model(inputs=inputs, outputs=conv6)\n",
    "    model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n",
    "    return model\n",
    "\n",
    "########################################################################################################\n",
    "#Attention R2U-Net\n",
    "def att_r2_unet(img_w, img_h, n_label, data_format='channels_first'):\n",
    "    inputs = Input(( img_w, img_h,3))\n",
    "    x = inputs\n",
    "    depth = 4\n",
    "    features = 32\n",
    "    skips = []\n",
    "    for i in range(depth):\n",
    "        x = rec_res_block(x, features, data_format=data_format)\n",
    "        skips.append(x)\n",
    "        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n",
    "\n",
    "        features = features * 2\n",
    "\n",
    "    x = rec_res_block(x, features, data_format=data_format)\n",
    "\n",
    "    for i in reversed(range(depth)):\n",
    "        features = features // 2\n",
    "        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n",
    "        x = rec_res_block(x, features, data_format=data_format)\n",
    "\n",
    "    conv6 = Conv2D(n_label, (1, 1), padding='same', activation=\"sigmoid\",data_format=data_format)(x)\n",
    "   \n",
    "    model = Model(inputs=inputs, outputs=conv6)\n",
    "    model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filenames = glob.glob('../../data/CycleGANData/wholeslide/image/*.tiff')\n",
    "mask_filenames=[f.replace('/image', '/mask') for f in mask_filenames]\n",
    "mask_df = pd.DataFrame()\n",
    "mask_df['filename'] = mask_filenames\n",
    "mask_df['mask_percentage'] = 0\n",
    "mask_df['labels'] = 0\n",
    "mask_df.set_index('filename', inplace=True)\n",
    "for file in mask_filenames:\n",
    "    mask_df.loc[file, 'mask_percentage'] = np.array(Image.open(file)).sum()/(512*512*255)\n",
    "mask_df.loc[mask_df.mask_percentage > 0.01, 'labels'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_filenames = glob.glob('../../data/CycleGANData/wholeslide/image/*.tiff')\n",
    "\n",
    "train_filenames, valid_filenames = train_test_split(train_valid_filenames,test_size = 0.2,train_size=0.8, random_state = 10)\n",
    "mask_train_filenames = [f.replace('/image', '/mask') for f in train_filenames]\n",
    "mask_valid_filenames = [f.replace('/image', '/mask') for f in valid_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((len(train_filenames),512,512,3))\n",
    "valid_x = np.zeros((len(valid_filenames),512,512,3))\n",
    "train_mask_y = np.zeros((len(train_filenames),512,512))\n",
    "valid_mask_y = np.zeros((len(valid_filenames),512,512))\n",
    "train_y = np.zeros((len(train_filenames)))\n",
    "valid_y = np.zeros((len(valid_filenames)))\n",
    "for (index, image) in enumerate(train_filenames[:]):\n",
    "    train_x[index] = (255-np.array(Image.open(image)))/255\n",
    "    \n",
    "# 검증용  및 변병여부 \n",
    "for (index, image) in enumerate(valid_filenames[:]):\n",
    "    valid_x[index] = (255-np.array(Image.open(image)))/255\n",
    "    \n",
    "# 훈련용 마스크 답지  및 병변여부 \n",
    "for (index, image) in enumerate(mask_train_filenames[:]):\n",
    "    train_mask_y[index] = np.array(Image.open(image))/255\n",
    "    train_y[index] = mask_df.loc[image, 'labels']\n",
    "    \n",
    "# 검증용 병변 답지  및 병변여부 \n",
    "for (index, image) in enumerate(mask_valid_filenames[:]):\n",
    "    valid_mask_y[index] = np.array(Image.open(image))/255\n",
    "    valid_y[index] = mask_df.loc[image, 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "227/227 [==============================] - 56s 169ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 2/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 3/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 4/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8543 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 5/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8545 - dice_coef: 0.1453 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 6/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 7/300\n",
      "227/227 [==============================] - 39s 173ms/step - loss: 0.8541 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 8/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 9/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 10/300\n",
      "227/227 [==============================] - 39s 172ms/step - loss: 0.8536 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 11/300\n",
      "227/227 [==============================] - 39s 170ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 12/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 13/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 14/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 15/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8537 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 16/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 17/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 18/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 19/300\n",
      "227/227 [==============================] - 39s 171ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 20/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 21/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 22/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 23/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 24/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 25/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 26/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8539 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 27/300\n",
      "227/227 [==============================] - 39s 172ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 28/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 29/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 30/300\n",
      "227/227 [==============================] - 38s 165ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 31/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 32/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 33/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1468 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 34/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 35/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8544 - dice_coef: 0.1454 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 36/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 37/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 38/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8542 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 39/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 40/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 41/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 42/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8542 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 43/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 44/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 45/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 46/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 47/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 48/300\n",
      "227/227 [==============================] - 39s 170ms/step - loss: 0.8539 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 49/300\n",
      "227/227 [==============================] - 39s 172ms/step - loss: 0.8541 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 50/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8545 - dice_coef: 0.1453 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 51/300\n",
      "227/227 [==============================] - 39s 171ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 52/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 53/300\n",
      "227/227 [==============================] - 37s 165ms/step - loss: 0.8540 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 54/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 55/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 56/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8543 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 57/300\n",
      "227/227 [==============================] - 38s 165ms/step - loss: 0.8538 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 58/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8545 - dice_coef: 0.1454 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 59/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 60/300\n",
      "227/227 [==============================] - 39s 172ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 61/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 62/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 63/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 64/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 65/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 66/300\n",
      "227/227 [==============================] - 39s 170ms/step - loss: 0.8536 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 67/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 68/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8542 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 69/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 70/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8539 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 71/300\n",
      "227/227 [==============================] - 39s 171ms/step - loss: 0.8540 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 72/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 73/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8546 - dice_coef: 0.1453 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 74/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 75/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 76/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 77/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 78/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 79/300\n",
      "227/227 [==============================] - 37s 165ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 80/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 81/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 82/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 83/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 84/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 85/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 86/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 87/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8544 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 88/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 89/300\n",
      "227/227 [==============================] - 39s 170ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 90/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 91/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 92/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8537 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 93/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 94/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 95/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 96/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8540 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 97/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 98/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 99/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 100/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8538 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 101/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 102/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 103/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 104/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 105/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 106/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 107/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 108/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 109/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 110/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 111/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 112/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 113/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 114/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 115/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 116/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8542 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 117/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 118/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 119/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 120/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8537 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 121/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8542 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 122/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 123/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 124/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 125/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 126/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8536 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 127/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 128/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 129/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 130/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 131/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 132/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 133/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 134/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 135/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 136/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8543 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 137/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 138/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 139/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 140/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8536 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 141/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8542 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 142/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 143/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 144/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 145/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 146/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8544 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 147/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 148/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 149/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 150/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 151/300\n",
      "227/227 [==============================] - 39s 171ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 152/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 153/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 154/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 155/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8543 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 156/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8544 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 157/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8536 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 158/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 159/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 160/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8539 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 161/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 162/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 163/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 164/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 165/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8537 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 166/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 167/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 168/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 169/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 170/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8541 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 171/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8537 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 172/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 173/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8539 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 174/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8543 - dice_coef: 0.1454 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 175/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 176/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 177/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8545 - dice_coef: 0.1454 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 178/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 179/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 180/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 181/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8537 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 182/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 183/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 184/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 185/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 186/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 187/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 188/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 189/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8537 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 190/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 191/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8542 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 192/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 193/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8544 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 194/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 195/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 196/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 197/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 198/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8540 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 199/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 200/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 201/300\n",
      "227/227 [==============================] - 38s 169ms/step - loss: 0.8536 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 202/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 203/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8537 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 204/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 205/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1466 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 206/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8537 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 207/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 208/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 209/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 210/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 211/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 212/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 213/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 214/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8543 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 215/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 216/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 217/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 218/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 219/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 220/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 221/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8546 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 222/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 223/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 224/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8539 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 225/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8542 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 226/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 227/300\n",
      "227/227 [==============================] - 38s 166ms/step - loss: 0.8539 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 228/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 229/300\n",
      "227/227 [==============================] - 37s 164ms/step - loss: 0.8536 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 230/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 231/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 232/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 233/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 234/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 235/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 236/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1467 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 237/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 238/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 239/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 240/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 241/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 242/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 243/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1456 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 244/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 245/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 246/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 247/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 248/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8539 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 249/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 250/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8537 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 251/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 252/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8537 - dice_coef: 0.1467 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 253/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 254/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 255/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 256/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 257/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 258/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8537 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 259/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8538 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 260/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 261/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 262/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8539 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 263/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 264/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 265/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1455 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 266/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8535 - dice_coef: 0.1468 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 267/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8538 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 268/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8538 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 269/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 270/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8538 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 271/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8536 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 272/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8538 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 273/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 274/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 275/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8543 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 276/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8541 - dice_coef: 0.1464 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 277/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8537 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 278/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 279/300\n",
      "227/227 [==============================] - 36s 161ms/step - loss: 0.8539 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 280/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 281/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8538 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 282/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8543 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 283/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8542 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 284/300\n",
      "227/227 [==============================] - 36s 160ms/step - loss: 0.8541 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 285/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 286/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1465 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 287/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1458 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 288/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8538 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 289/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8539 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 290/300\n",
      "227/227 [==============================] - 37s 161ms/step - loss: 0.8542 - dice_coef: 0.1461 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 291/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8537 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 292/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 293/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8541 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 294/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8542 - dice_coef: 0.1457 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 295/300\n",
      "227/227 [==============================] - 37s 163ms/step - loss: 0.8540 - dice_coef: 0.1462 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 296/300\n",
      "227/227 [==============================] - 36s 159ms/step - loss: 0.8540 - dice_coef: 0.1459 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 297/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8538 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 298/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8540 - dice_coef: 0.1460 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 299/300\n",
      "227/227 [==============================] - 36s 158ms/step - loss: 0.8546 - dice_coef: 0.1453 - val_loss: 0.8532 - val_dice_coef: 0.1464\n",
      "Epoch 300/300\n",
      "227/227 [==============================] - 37s 162ms/step - loss: 0.8538 - dice_coef: 0.1463 - val_loss: 0.8532 - val_dice_coef: 0.1464\n"
     ]
    }
   ],
   "source": [
    "model = att_r2_unet(512,512,1,data_format='channels_last')\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=dice_coef_loss, metrics=dice_coef)\n",
    "callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"../../model/Tumor_segmentation_Attention_{epoch:03d}.h5\", monitor='val_loss'),\n",
    "    ]\n",
    "history=model.fit(train_x, [train_mask_y], validation_data=(valid_x, [valid_mask_y]),callbacks=callbacks,epochs = 300, batch_size =4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
