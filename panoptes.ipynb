{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.models as models \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm_notebook\n",
    "device0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_glob, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_list = glob(img_glob)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.file_name_list=[]\n",
    "        self.file_remove_list=[]\n",
    "        self.label_list=[]\n",
    "        \n",
    "        for i in range(len(self.img_list)):\n",
    "            self.file_name_list.append(os.path.splitext(os.path.basename(self.img_list[i]))[0][:9])\n",
    "\n",
    "        for i in range(len(self.file_name_list)):\n",
    "            label_index=self.img_labels.loc[self.img_labels['PathologyNumber'] == self.file_name_list[i]]\n",
    "            if len(label_index)==0:\n",
    "                self.file_remove_list.append(self.img_list[i])\n",
    "            else:\n",
    "                if label_index['MMR status'].to_list()[0]=='Normal':\n",
    "                    self.label_list.append(0)\n",
    "                else:\n",
    "                    self.label_list.append(1)\n",
    "        for i in range(len(self.file_remove_list)):\n",
    "            self.img_list.remove(self.file_remove_list[i])\n",
    "        self.image_x5=[f.replace('/2.5x_standard', '/5x_standard') for f in self.img_list]\n",
    "        self.image_x10=[f.replace('/2.5x_standard', '/10x_standard') for f in self.img_list]\n",
    "        img_5x_temp=[]\n",
    "        img_10x_temp=[]\n",
    "        for i in range(len(self.img_list)):\n",
    "            for j in range(4):\n",
    "                img_5x_temp.append(self.image_x5[i][:-4]+'_'+str(j)+'.jpg')\n",
    "            for j in range(16):\n",
    "                img_10x_temp.append(self.image_x10[i][:-4]+'_'+str(j)+'.jpg')\n",
    "            self.image_x5[i]=img_5x_temp\n",
    "            self.image_x10[i]=img_10x_temp\n",
    "        self.transform = T.Resize(224)\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transform(read_image(self.img_list[idx]))/255\n",
    "        label = self.label_list[idx]\n",
    "        image_5x=torch.zeros(4,3,224,224)\n",
    "        image_10x=torch.zeros(16,3,224,224)\n",
    "        for i in range(4):\n",
    "            image_5x[i]=self.transform(read_image(self.image_x5[idx][i]))\n",
    "        for i in range(16):\n",
    "            image_10x[i]=self.transform(read_image(self.image_x10[idx][i]))\n",
    "        image_5x=np.transpose(image_5x, axes=(1, 0, 2,3))/255\n",
    "        image_10x=np.transpose(image_10x, axes=(1, 0, 2,3))/255\n",
    "        image_5x=torch.reshape(image_5x,(3*4,224,224))\n",
    "        image_10x=torch.reshape(image_10x,(16*3,224,224))\n",
    "        \n",
    "        return image,image_5x,image_10x,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_glob='../../data/CycleGANData/3divisionTile/2.5x_standard/*.jpg'\n",
    "annotations_file='../../data/OriginalData/MMR.csv'\n",
    "dataset=CustomImageDataset(annotations_file,img_glob)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=24, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panoptes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Panoptes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_25x=EfficientNet.from_name('efficientnet-b2', in_channels=3).to(device0)\n",
    "        self.model_5x=EfficientNet.from_name('efficientnet-b2', in_channels=3*4).to(device0)\n",
    "        self.model_10x=EfficientNet.from_name('efficientnet-b2', in_channels=3*16).to(device0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(3000, 300).to(device0)\n",
    "        self.fc2 = nn.Linear(300, 30).to(device0)\n",
    "        self.fc3 = nn.Linear(30, 1).to(device0)\n",
    "        self.cat1=torch.cat\n",
    "        self.cat2=torch.cat\n",
    "        \n",
    "    def forward(self, x25,x5,x10):\n",
    "        x25 = self.model_25x(x25).to(device0)\n",
    "        x5 = self.model_5x(x5).to(device0)\n",
    "        x10 = self.model_10x(x10).to(device0)\n",
    "        x = self.cat1((x25, x5), dim=1).to(device0)\n",
    "        x = self.cat2((x, x10), dim=1).to(device0)\n",
    "        x = self.relu(self.fc1(x).to(device0))\n",
    "        x = self.relu(self.fc2(x).to(device0))\n",
    "        x = self.fc3(x).to(device0)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3042426/4186111025.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  train=tqdm_notebook(train_dataloader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5454007c5c5444579ca8b7c22b3bc1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3042426/4186111025.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  val=tqdm_notebook(validation_dataloader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3f056212d0466987cf7f5e99a87333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.39 GiB total capacity; 37.66 GiB already allocated; 1.31 MiB free; 37.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device0)\n\u001b[1;32m     29\u001b[0m count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 30\u001b[0m output \u001b[39m=\u001b[39m model(x25\u001b[39m.\u001b[39;49mto(device0),x5\u001b[39m.\u001b[39;49mto(device0),x10\u001b[39m.\u001b[39;49mto(device0))\u001b[39m.\u001b[39mto(device0)\n\u001b[1;32m     31\u001b[0m z\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39munsqueeze(torch\u001b[39m.\u001b[39msqueeze(y),\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtype(\u001b[39m'\u001b[39m\u001b[39mtorch.cuda.FloatTensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m k\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39munsqueeze(torch\u001b[39m.\u001b[39msqueeze(output),\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mPanoptes.forward\u001b[0;34m(self, x25, x5, x10)\u001b[0m\n\u001b[1;32m     15\u001b[0m x25 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_25x(x25)\u001b[39m.\u001b[39mto(device0)\n\u001b[1;32m     16\u001b[0m x5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_5x(x5)\u001b[39m.\u001b[39mto(device0)\n\u001b[0;32m---> 17\u001b[0m x10 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_10x(x10)\u001b[39m.\u001b[39mto(device0)\n\u001b[1;32m     18\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat1((x25, x5), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device0)\n\u001b[1;32m     19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat2((x, x10), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device0)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/efficientnet_pytorch/model.py:320\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    319\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dropout(x)\n\u001b[0;32m--> 320\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fc(x)\n\u001b[1;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.39 GiB total capacity; 37.66 GiB already allocated; 1.31 MiB free; 37.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = Panoptes().to(device0)\n",
    "criterion = nn.BCELoss().to(device0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    cost = 0.0\n",
    "    train=tqdm_notebook(train_dataloader)\n",
    "    count=0\n",
    "    TP_count=1\n",
    "    FP_count=1\n",
    "    TN_count=1\n",
    "    FN_count=1\n",
    "    for x25,x5,x10, y in train:\n",
    "        y = y.to(device0)\n",
    "        count+=1\n",
    "        output = model(x25.to(device0),x5.to(device0),x10.to(device0))\n",
    "        loss = criterion(torch.unsqueeze(torch.squeeze(output),0), torch.unsqueeze(torch.squeeze(y),0).type('torch.cuda.FloatTensor')).to(device0)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost += loss\n",
    "        train.set_description(f\"epoch: {epoch+1}/{10000} Step: {count+1} loss :{(cost/count):.4f}\")\n",
    "        \n",
    "    val=tqdm_notebook(validation_dataloader)\n",
    "    count=0\n",
    "    for x25,x5,x10, y in val:\n",
    "        \n",
    "        y = y.to(device0)\n",
    "        count+=1\n",
    "        output = model(x25.to(device0),x5.to(device0),x10.to(device0)).to(device0)\n",
    "        z=torch.unsqueeze(torch.squeeze(y),0).type('torch.cuda.FloatTensor')\n",
    "        k=torch.unsqueeze(torch.squeeze(output),0)\n",
    "        loss = criterion(k, z).to(device0)\n",
    "        optimizer.zero_grad()\n",
    "        if z==0:\n",
    "            if k>=0.5:\n",
    "                FP_count+=1\n",
    "            else:\n",
    "                TN_count+=1\n",
    "        else:\n",
    "            if k>=0.5:\n",
    "                TP_count+=1\n",
    "            else:\n",
    "                FN_count+=1\n",
    "        cost += loss\n",
    "        val.set_description(f\"loss : {cost/count} negative_accuracy: {TN_count/(FP_count+TN_count):.4f} positive_accuracy: {TP_count/(TP_count+FN_count):.4f}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z==0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
