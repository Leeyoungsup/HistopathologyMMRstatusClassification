{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.stats import ttest_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionResNetV2_cm=np.array([[36,16],[55,127]])\n",
    "efficientNetB2_cm=np.array([[43,9],[43,139]])\n",
    "efficientNetB3_cm=np.array([[37,15],[37,145]])\n",
    "y_test=np.ones([52])\n",
    "y_test=np.concatenate([y_test,np.zeros([182])])\n",
    "inceptionResNetV2_pred=np.zeros([len(y_test)])\n",
    "inceptionResNetV2_pred[:inceptionResNetV2_cm[0,0]]=1\n",
    "inceptionResNetV2_pred[52:52+inceptionResNetV2_cm[1,0]]=1\n",
    "efficientNetB2_pred=np.zeros([len(y_test)])\n",
    "efficientNetB2_pred[:efficientNetB2_cm[0,0]]=1\n",
    "efficientNetB2_pred[52:52+efficientNetB2_cm[1,0]]=1\n",
    "efficientNetB3_pred=np.zeros([len(y_test)])\n",
    "efficientNetB3_pred[:efficientNetB3_cm[0,0]]=1\n",
    "efficientNetB3_pred[52:52+efficientNetB3_cm[1,0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model    Metric     Value  Alpha  \\\n",
      "0   Model 1  Accuracy  0.696581  0.200   \n",
      "0   Model 1  Accuracy  0.696581  0.150   \n",
      "0   Model 1  Accuracy  0.696581  0.100   \n",
      "0   Model 1  Accuracy  0.696581  0.050   \n",
      "0   Model 1  Accuracy  0.696581  0.025   \n",
      "..      ...       ...       ...    ...   \n",
      "0   Model 3  F1 Score  0.587302  0.050   \n",
      "0   Model 3  F1 Score  0.587302  0.025   \n",
      "0   Model 3  F1 Score  0.587302  0.010   \n",
      "0   Model 3  F1 Score  0.587302  0.005   \n",
      "0   Model 3  F1 Score  0.587302  0.001   \n",
      "\n",
      "                         Confidence Interval   p-value  \\\n",
      "0   (0.6556236315641998, 0.7375387615981933)  0.000083   \n",
      "0   (0.6505534040262204, 0.7426089891361727)  0.000083   \n",
      "0   (0.6439525289011624, 0.7492098642612307)  0.000083   \n",
      "0   (0.6337934763835406, 0.7593689167788525)  0.000083   \n",
      "0   (0.6246859388785894, 0.7684764542838038)  0.000083   \n",
      "..                                       ...       ...   \n",
      "0    (0.527410547862424, 0.6471926267407504)  0.021846   \n",
      "0   (0.5187231821452809, 0.6558799924578936)  0.021846   \n",
      "0   (0.5083538395105466, 0.6662493350926278)  0.021846   \n",
      "0    (0.501151784003458, 0.6734513905997167)  0.021846   \n",
      "0   (0.4859903739564121, 0.6886128006467633)  0.021846   \n",
      "\n",
      "   ANOVA with bonferroni collection p-value  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "..                                      ...  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "0                      [0.2424619337408715]  \n",
      "\n",
      "[96 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3012593/840752137.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import t, ttest_ind, f_oneway\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 가상의 예제 데이터 생성\n",
    "data_model1 = inceptionResNetV2_pred\n",
    "data_model2 = efficientNetB2_pred\n",
    "data_model3 = efficientNetB3_pred\n",
    "\n",
    "# 이진분류 모델 3개의 결과를 각각 저장\n",
    "results_model1 = data_model1 > 0\n",
    "results_model2 = data_model2 > 0\n",
    "results_model3 = data_model3 > 0\n",
    "\n",
    "true_labels = y_test\n",
    "\n",
    "# 성능 지표 계산 함수\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, sensitivity, specificity, f1\n",
    "\n",
    "alpha_values = [0.2, 0.15, 0.1, 0.05, 0.025, 0.01, 0.005, 0.001]\n",
    "\n",
    "# 결과를 저장할 DataFrame 생성\n",
    "result_df = pd.DataFrame(columns=['Model', 'Metric', 'Value','Alpha', 'Confidence Interval', 'p-value', 'ANOVA with bonferroni collection p-value'])\n",
    "\n",
    "# 각 모델에 대한 신뢰구간, p-value, ANOVA p-value 계산 및 DataFrame에 저장\n",
    "for model, results in zip(['Model 1', 'Model 2', 'Model 3'], [results_model1, results_model2, results_model3]):\n",
    "    accuracy, sensitivity, specificity, f1 = calculate_performance_metrics(true_labels, results)\n",
    "    \n",
    "    # Calculate p-value using t-test\n",
    "    t_statistic, p_value_ttest = ttest_ind(results, true_labels)\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    _, p_value_anova = f_oneway(results_model1, results_model2, results_model3)\n",
    "    \n",
    "    # Apply Bonferroni correction to ANOVA p-value\n",
    "    _, p_value_anova_corrected, _, _ = multipletests([p_value_anova], method='bonferroni')\n",
    "    \n",
    "    for metric_name, metric_value in zip(['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score'], [accuracy, sensitivity, specificity, f1]):\n",
    "        for alpha in alpha_values:\n",
    "            # t.interval 함수를 사용하여 신뢰구간 계산\n",
    "            df = len(results) - 1\n",
    "            confidence_interval = t.interval(1 - alpha, df, loc=metric_value, scale=np.std(results) / np.sqrt(len(results)))\n",
    "            \n",
    "            # 결과를 DataFrame에 추가\n",
    "            result_df = pd.concat([result_df, pd.DataFrame({\n",
    "                'Model': [model],\n",
    "                'Metric': [metric_name],\n",
    "                'Value': [metric_value],\n",
    "                'Alpha':[alpha],\n",
    "                'Confidence Interval': [confidence_interval],\n",
    "                'p-value': [p_value_ttest],\n",
    "                'ANOVA with bonferroni collection p-value': [p_value_anova_corrected]\n",
    "            })])\n",
    "\n",
    "# 결과 DataFrame 출력\n",
    "print(result_df)\n",
    "# 결과 DataFrame을 CSV 파일로 저장 (필요에 따라 주석 처리)\n",
    "result_df.to_csv('model_comparison_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model    Metric     Value  Alpha  \\\n",
      "0    Model 1  Accuracy  0.696581  0.200   \n",
      "1    Model 1  Accuracy  0.696581  0.150   \n",
      "2    Model 1  Accuracy  0.696581  0.100   \n",
      "3    Model 1  Accuracy  0.696581  0.050   \n",
      "4    Model 1  Accuracy  0.696581  0.025   \n",
      "..       ...       ...       ...    ...   \n",
      "115  Model 3       AUC  0.754121  0.050   \n",
      "116  Model 3       AUC  0.754121  0.025   \n",
      "117  Model 3       AUC  0.754121  0.010   \n",
      "118  Model 3       AUC  0.754121  0.005   \n",
      "119  Model 3       AUC  0.754121  0.001   \n",
      "\n",
      "                          Confidence Interval   p-value  \\\n",
      "0    (0.6556236315641998, 0.7375387615981933)  0.000083   \n",
      "1    (0.6505534040262204, 0.7426089891361727)  0.000083   \n",
      "2    (0.6439525289011624, 0.7492098642612307)  0.000083   \n",
      "3    (0.6337934763835406, 0.7593689167788525)  0.000083   \n",
      "4    (0.6246859388785894, 0.7684764542838038)  0.000083   \n",
      "..                                        ...       ...   \n",
      "115   (0.694229839681716, 0.8140119185600424)  0.021846   \n",
      "116  (0.6855424739645728, 0.8226992842771856)  0.021846   \n",
      "117  (0.6751731313298386, 0.8330686269119199)  0.021846   \n",
      "118    (0.66797107582275, 0.8402706824190087)  0.021846   \n",
      "119  (0.6528096657757041, 0.8554320924660553)  0.021846   \n",
      "\n",
      "    ANOVA with bonferroni collection p-value  DeLong p-value  \n",
      "0                       [0.2424619337408715]        0.675373  \n",
      "1                       [0.2424619337408715]        0.675373  \n",
      "2                       [0.2424619337408715]        0.675373  \n",
      "3                       [0.2424619337408715]        0.675373  \n",
      "4                       [0.2424619337408715]        0.675373  \n",
      "..                                       ...             ...  \n",
      "115                     [0.2424619337408715]        0.713728  \n",
      "116                     [0.2424619337408715]        0.713728  \n",
      "117                     [0.2424619337408715]        0.713728  \n",
      "118                     [0.2424619337408715]        0.713728  \n",
      "119                     [0.2424619337408715]        0.713728  \n",
      "\n",
      "[120 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import t, ttest_ind, f_oneway\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils import check_array\n",
    "from scipy.stats import norm\n",
    "# DeLong method for comparing ROC curves\n",
    "def delong_roc_test(y_true, y_scores1, y_scores2):\n",
    "    fpr1, tpr1, _ = roc_curve(y_true, y_scores1)\n",
    "    fpr2, tpr2, _ = roc_curve(y_true, y_scores2)\n",
    "\n",
    "    auc1 = auc(fpr1, tpr1)\n",
    "    auc2 = auc(fpr2, tpr2)\n",
    "    roc_distances = pairwise_distances(\n",
    "        y_true.reshape(-1, 1), y_scores1.reshape(-1, 1), metric=\"manhattan\"\n",
    "    ) - pairwise_distances(y_true.reshape(-1, 1), y_scores2.reshape(-1, 1), metric=\"manhattan\")\n",
    "\n",
    "    delong_variance = np.var(roc_distances, ddof=2) / 4.0\n",
    "\n",
    "    delong_se = np.sqrt(delong_variance)\n",
    "\n",
    "    z_score = (auc1 - auc2) / delong_se\n",
    "\n",
    "    p_value = 2 * (1.0 - norm.cdf(np.abs(z_score)))\n",
    "\n",
    "    return z_score, p_value\n",
    "\n",
    "# 가상의 예제 데이터 생성\n",
    "data_model1 = inceptionResNetV2_pred\n",
    "data_model2 = efficientNetB2_pred\n",
    "data_model3 = efficientNetB3_pred\n",
    "\n",
    "# 이진분류 모델 3개의 결과를 각각 저장\n",
    "results_model1 = data_model1 > 0\n",
    "results_model2 = data_model2 > 0\n",
    "results_model3 = data_model3 > 0\n",
    "\n",
    "true_labels = y_test\n",
    "result_list = []\n",
    "# 성능 지표 계산 함수\n",
    "def calculate_performance_metrics(y_true, y_pred, y_scores):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc_value = roc_auc_score(y_true, y_scores)\n",
    "    return accuracy, sensitivity, specificity, f1, auc_value\n",
    "\n",
    "alpha_values = [0.2, 0.15, 0.1, 0.05, 0.025, 0.01, 0.005, 0.001]\n",
    "\n",
    "# 결과를 저장할 DataFrame 생성\n",
    "result_df = pd.DataFrame(columns=['Model', 'Metric', 'Value', 'Alpha', 'Confidence Interval', 'p-value', 'ANOVA with bonferroni collection p-value', 'DeLong p-value'])\n",
    "\n",
    "# 각 모델에 대한 신뢰구간, p-value, ANOVA p-value, DeLong p-value 계산 및 리스트에 저장\n",
    "for model, results, data_model in zip(['Model 1', 'Model 2', 'Model 3'], [results_model1, results_model2, results_model3], [data_model1, data_model2, data_model3]):\n",
    "    accuracy, sensitivity, specificity, f1, auc_value = calculate_performance_metrics(true_labels, results, data_model)\n",
    "    \n",
    "    # Calculate p-value using t-test\n",
    "    t_statistic, p_value_ttest = ttest_ind(results, true_labels)\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    _, p_value_anova = f_oneway(results_model1, results_model2, results_model3)\n",
    "    \n",
    "    # Apply Bonferroni correction to ANOVA p-value\n",
    "    _, p_value_anova_corrected, _, _ = multipletests([p_value_anova], method='bonferroni')\n",
    "    \n",
    "    for metric_name, metric_value in zip(['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score', 'AUC'], [accuracy, sensitivity, specificity, f1, auc_value]):\n",
    "        for alpha in alpha_values:\n",
    "            # t.interval 함수를 사용하여 신뢰구간 계산\n",
    "            df = len(results) - 1\n",
    "            confidence_interval = t.interval(1 - alpha, df, loc=metric_value, scale=np.std(results) / np.sqrt(len(results)))\n",
    "            \n",
    "            # 결과를 리스트에 추가\n",
    "            result_list.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric_name,\n",
    "                'Value': metric_value,\n",
    "                'Alpha': alpha,\n",
    "                'Confidence Interval': confidence_interval,\n",
    "                'p-value': p_value_ttest,\n",
    "                'ANOVA with bonferroni collection p-value': p_value_anova_corrected,\n",
    "                'DeLong p-value': np.nan  # Placeholder for DeLong p-value\n",
    "            })\n",
    "\n",
    "# Calculate DeLong p-value for pairwise comparisons\n",
    "for i in range(len(result_list)):\n",
    "    for j in range(i + 1, len(result_list)):\n",
    "        if result_list[i]['Metric'] == result_list[j]['Metric'] and result_list[i]['Alpha'] == result_list[j]['Alpha']:\n",
    "            metric_name = result_list[i]['Metric']\n",
    "            alpha = result_list[i]['Alpha']\n",
    "            model1 = result_list[i]['Model']\n",
    "            model2 = result_list[j]['Model']\n",
    "            y_scores1 = data_model1 if model1 == 'Model 1' else (data_model2 if model1 == 'Model 2' else data_model3)\n",
    "            y_scores2 = data_model1 if model2 == 'Model 1' else (data_model2 if model2 == 'Model 2' else data_model3)\n",
    "            _, delong_p_value = delong_roc_test(true_labels, y_scores1, y_scores2)\n",
    "            result_list[i]['DeLong p-value'] = delong_p_value\n",
    "            result_list[j]['DeLong p-value'] = delong_p_value\n",
    "\n",
    "# 결과 리스트로부터 DataFrame 생성\n",
    "result_df = pd.DataFrame(result_list)\n",
    "\n",
    "# 결과 DataFrame 출력\n",
    "print(result_df)\n",
    "# 결과 DataFrame을 CSV 파일로 저장 (필요에 따라 주석 처리)\n",
    "result_df.to_csv('model_comparison_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Accuracy\n",
       "0    Accuracy\n",
       "0    Accuracy\n",
       "0    Accuracy\n",
       "0    Accuracy\n",
       "       ...   \n",
       "0         AUC\n",
       "0         AUC\n",
       "0         AUC\n",
       "0         AUC\n",
       "0         AUC\n",
       "Name: Metric, Length: 120, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[0, 'Metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T-test and p-value for Model 1:\n",
      "  Accuracy: 46.00%\n",
      "    t-statistic: -10.780417028313323, p-value: 1.2866965296964952e-21\n",
      "  Precision: 100.00%\n",
      "    t-statistic: -10.780417028313323, p-value: 1.2866965296964952e-21\n",
      "  Recall: 46.00%\n",
      "    t-statistic: -10.780417028313323, p-value: 1.2866965296964952e-21\n",
      "  F1 Score: 63.01%\n",
      "    t-statistic: -10.780417028313323, p-value: 1.2866965296964952e-21\n",
      "\n",
      "T-test and p-value for Model 2:\n",
      "  Accuracy: 68.00%\n",
      "    t-statistic: -6.825557507934252, p-value: 1.0449223161892874e-10\n",
      "  Precision: 100.00%\n",
      "    t-statistic: -6.825557507934252, p-value: 1.0449223161892874e-10\n",
      "  Recall: 68.00%\n",
      "    t-statistic: -6.825557507934252, p-value: 1.0449223161892874e-10\n",
      "  F1 Score: 80.95%\n",
      "    t-statistic: -6.825557507934252, p-value: 1.0449223161892874e-10\n",
      "\n",
      "T-test and p-value for Model 3:\n",
      "  Accuracy: 85.00%\n",
      "    t-statistic: -4.179783276115416, p-value: 4.377094840750872e-05\n",
      "  Precision: 100.00%\n",
      "    t-statistic: -4.179783276115416, p-value: 4.377094840750872e-05\n",
      "  Recall: 85.00%\n",
      "    t-statistic: -4.179783276115416, p-value: 4.377094840750872e-05\n",
      "  F1 Score: 91.89%\n",
      "    t-statistic: -4.179783276115416, p-value: 4.377094840750872e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t, ttest_ind\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 가상의 예제 데이터 생성\n",
    "np.random.seed(42)\n",
    "data_model1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "data_model2 = np.random.normal(loc=0.5, scale=1, size=100)\n",
    "data_model3 = np.random.normal(loc=1, scale=1, size=100)\n",
    "\n",
    "# 이진분류 모델 3개의 결과를 각각 저장\n",
    "results_model1 = (data_model1 > 0).astype(int)\n",
    "results_model2 = (data_model2 > 0).astype(int)\n",
    "results_model3 = (data_model3 > 0).astype(int)\n",
    "\n",
    "# 실제 라벨 (예시로 모두 1로 설정)\n",
    "true_labels = np.ones_like(data_model1)\n",
    "\n",
    "# 성능 지표 계산 함수\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# 각 모델에 대한 t-test 및 p-value 계산 및 출력\n",
    "for model, results in zip(['Model 1', 'Model 2', 'Model 3'], [results_model1, results_model2, results_model3]):\n",
    "    accuracy, precision, recall, f1 = calculate_performance_metrics(true_labels, results)\n",
    "    \n",
    "    print(f\"\\nT-test and p-value for {model}:\")\n",
    "    for metric_name, metric_value in zip(['Accuracy', 'Precision', 'Recall', 'F1 Score'], [accuracy, precision, recall, f1]):\n",
    "        print(f\"  {metric_name}: {metric_value:.2%}\")\n",
    "        t_statistic, p_value = ttest_ind(results, true_labels)\n",
    "        print(f\"    t-statistic: {t_statistic}, p-value: {p_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
